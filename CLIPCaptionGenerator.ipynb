{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2708faac65d4b1c9e88711d5a585647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dbe175edd7346aa9c050ae685c8248b",
              "IPY_MODEL_8bb72195cf5d4232a5671c0ed91a5e19",
              "IPY_MODEL_e5f0e4b9efbd4a0895ba110868528597"
            ],
            "layout": "IPY_MODEL_96dab87db2c74154aa4f42878594ce86"
          }
        },
        "5dbe175edd7346aa9c050ae685c8248b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfbf2adf535940bfb545ba6dc25542af",
            "placeholder": "​",
            "style": "IPY_MODEL_b9f2b92f7f3145298fb14e668917d617",
            "value": "Fetching 1 files: 100%"
          }
        },
        "8bb72195cf5d4232a5671c0ed91a5e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5af2a20633b4231b77a09d6a1b22186",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a71f3e46ed1f4abca16c864e5bcd5b22",
            "value": 1
          }
        },
        "e5f0e4b9efbd4a0895ba110868528597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6c4a7f93ae4cafadcf8a361e24d58c",
            "placeholder": "​",
            "style": "IPY_MODEL_8db41a899e044d8881981721af33d16e",
            "value": " 1/1 [00:00&lt;00:00, 29.21it/s]"
          }
        },
        "96dab87db2c74154aa4f42878594ce86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfbf2adf535940bfb545ba6dc25542af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f2b92f7f3145298fb14e668917d617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5af2a20633b4231b77a09d6a1b22186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71f3e46ed1f4abca16c864e5bcd5b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c6c4a7f93ae4cafadcf8a361e24d58c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db41a899e044d8881981721af33d16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d90d38e0fa848409db0d6cf3dfafaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1d9ed6c25be45dd89ef6f327d8d82da",
              "IPY_MODEL_e04966c72a9441929438ac2cefc77184",
              "IPY_MODEL_0be9264b8f084a6680fb0703cebb39dc"
            ],
            "layout": "IPY_MODEL_9e9e0792daf14383ab2445bfe60cbd7b"
          }
        },
        "a1d9ed6c25be45dd89ef6f327d8d82da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eb5fb7c07a14622a9f95b6f5580ddad",
            "placeholder": "​",
            "style": "IPY_MODEL_73b745faad5d4e269398c228f4199cf5",
            "value": "Fetching 1 files: 100%"
          }
        },
        "e04966c72a9441929438ac2cefc77184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e6eb70f5bd1499bb07f41b68a48ca0f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0b5a6342269410abc458b8234d6c314",
            "value": 1
          }
        },
        "0be9264b8f084a6680fb0703cebb39dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8583597c58f6477fbefe54765abc347f",
            "placeholder": "​",
            "style": "IPY_MODEL_4d25049ac510490fab4be9d53a62d11d",
            "value": " 1/1 [00:00&lt;00:00, 70.78it/s]"
          }
        },
        "9e9e0792daf14383ab2445bfe60cbd7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eb5fb7c07a14622a9f95b6f5580ddad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73b745faad5d4e269398c228f4199cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e6eb70f5bd1499bb07f41b68a48ca0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0b5a6342269410abc458b8234d6c314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8583597c58f6477fbefe54765abc347f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d25049ac510490fab4be9d53a62d11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1BEGNgBtrGB",
        "outputId": "7c96781a-d434-4969-a93c-fbecce8444ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch torchvision PIL requests gradio accelerate -q\n",
        "!pip install git+https://github.com/openai/CLIP.git -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch\n",
        "import clip\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import gradio as gr\n",
        "import io\n",
        "import base64\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFWBjZ2xt4VG",
        "outputId": "467e8a85-3469-4f92-d21c-2ffe6ec164f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# Define Main Caption Generator Class\n",
        "# ====================================\n",
        "class ImageCaptionGenerator:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.processors = {}\n",
        "        self.load_models()\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load multiple caption generation models\"\"\"\n",
        "        print(\"Loading models... This may take a few minutes on first run.\")\n",
        "\n",
        "        # 1. BLIP Model (Salesforce) - State of the art\n",
        "        print(\"Loading BLIP model...\")\n",
        "        self.processors[\"blip\"] = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "        self.models[\"blip\"] = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
        "\n",
        "        # 2. ViT-GPT2 Model - Good for detailed descriptions\n",
        "        print(\"Loading ViT-GPT2 model...\")\n",
        "        self.models[\"vit_gpt2\"] = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
        "        self.processors[\"vit_gpt2_feature\"] = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "        self.processors[\"vit_gpt2_tokenizer\"] = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "        # 3. CLIP Model - For similarity and context\n",
        "        print(\"Loading CLIP model...\")\n",
        "        self.models[\"clip\"], self.processors[\"clip\"] = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "        print(\"All models loaded successfully!\")\n",
        "\n",
        "\n",
        "# ====================================\n",
        "#  BLIP Caption Generation Methods\n",
        "# ====================================\n",
        "    def generate_blip_caption(self, image: Image.Image, mode: str = \"normal\") -> str:\n",
        "        \"\"\"Generate caption using BLIP model\"\"\"\n",
        "        try:\n",
        "            if mode == \"conditional\":\n",
        "                # Conditional generation with prompt\n",
        "                text = \"a photography of\"\n",
        "                inputs = self.processors[\"blip\"](image, text, return_tensors=\"pt\").to(device)\n",
        "            else:\n",
        "                # Unconditional generation\n",
        "                inputs = self.processors[\"blip\"](image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = self.models[\"blip\"].generate(**inputs, max_length=50, num_beams=5)\n",
        "\n",
        "            caption = self.processors[\"blip\"].decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # Clean up conditional prompt if used\n",
        "            if mode == \"conditional\" and caption.startswith(\"a photography of\"):\n",
        "                caption = caption.replace(\"a photography of\", \"\").strip()\n",
        "\n",
        "            return caption\n",
        "        except Exception as e:\n",
        "            return f\"Error generating BLIP caption: {str(e)}\"\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# ViT-GPT2 Caption Generation Method\n",
        "# ====================================\n",
        "    def generate_vit_gpt2_caption(self, image: Image.Image) -> str:\n",
        "        \"\"\"Generate caption using ViT-GPT2 model\"\"\"\n",
        "        try:\n",
        "            # Preprocess image\n",
        "            pixel_values = self.processors[\"vit_gpt2_feature\"](\n",
        "                images=image, return_tensors=\"pt\"\n",
        "            ).pixel_values.to(device)\n",
        "\n",
        "            # Generate caption\n",
        "            with torch.no_grad():\n",
        "                output_ids = self.models[\"vit_gpt2\"].generate(\n",
        "                    pixel_values,\n",
        "                    max_length=50,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            # Decode caption\n",
        "            caption = self.processors[\"vit_gpt2_tokenizer\"].decode(\n",
        "                output_ids[0], skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "            return caption.strip()\n",
        "        except Exception as e:\n",
        "            return f\"Error generating ViT-GPT2 caption: {str(e)}\"\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# CLIP Analysis Method\n",
        "# ====================================\n",
        "    def analyze_with_clip(self, image: Image.Image, text_options: List[str]) -> Tuple[str, List[float]]:\n",
        "        \"\"\"Use CLIP to find best matching description\"\"\"\n",
        "        try:\n",
        "            # Preprocess\n",
        "            image_input = self.processors[\"clip\"](image).unsqueeze(0).to(device)\n",
        "            text_inputs = clip.tokenize(text_options).to(device)\n",
        "\n",
        "            # Get features\n",
        "            with torch.no_grad():\n",
        "                image_features = self.models[\"clip\"].encode_image(image_input)\n",
        "                text_features = self.models[\"clip\"].encode_text(text_inputs)\n",
        "\n",
        "                # Calculate similarity\n",
        "                similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "                similarities = similarities.cpu().numpy()[0]\n",
        "\n",
        "            # Find best match\n",
        "            best_idx = np.argmax(similarities)\n",
        "            best_description = text_options[best_idx]\n",
        "\n",
        "            return best_description, similarities.tolist()\n",
        "        except Exception as e:\n",
        "            return f\"Error with CLIP analysis: {str(e)}\", []\n",
        "\n",
        "\n",
        "# ====================================\n",
        "#  Comprehensive Caption Generation\n",
        "# ====================================\n",
        "    def generate_comprehensive_caption(self, image: Image.Image) -> dict:\n",
        "        \"\"\"Generate multiple captions using different approaches\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # BLIP captions\n",
        "        print(\"Generating BLIP captions...\")\n",
        "        results[\"blip_basic\"] = self.generate_blip_caption(image, \"normal\")\n",
        "        results[\"blip_conditional\"] = self.generate_blip_caption(image, \"conditional\")\n",
        "\n",
        "        # ViT-GPT2 caption\n",
        "        print(\"Generating ViT-GPT2 caption...\")\n",
        "        results[\"vit_gpt2\"] = self.generate_vit_gpt2_caption(image)\n",
        "\n",
        "        # CLIP-based scene analysis\n",
        "        print(\"Analyzing scene with CLIP...\")\n",
        "        scene_options = [\n",
        "            \"a photo taken indoors\",\n",
        "            \"a photo taken outdoors\",\n",
        "            \"a portrait of a person\",\n",
        "            \"a landscape scene\",\n",
        "            \"an urban environment\",\n",
        "            \"a natural environment\",\n",
        "            \"an animal\",\n",
        "            \"food or cooking\",\n",
        "            \"a vehicle\",\n",
        "            \"architecture or buildings\",\n",
        "            \"art or creative work\",\n",
        "            \"sports or activity\"\n",
        "        ]\n",
        "\n",
        "        clip_scene, scene_scores = self.analyze_with_clip(image, scene_options)\n",
        "        results[\"clip_scene\"] = clip_scene\n",
        "        results[\"scene_confidence\"] = max(scene_scores) if scene_scores else 0\n",
        "\n",
        "        # Generate enhanced caption by combining insights\n",
        "        print(\"Creating enhanced caption...\")\n",
        "        enhanced_caption = self.create_enhanced_caption(results)\n",
        "        results[\"enhanced\"] = enhanced_caption\n",
        "\n",
        "        return results\n",
        "\n",
        "    def create_enhanced_caption(self, results: dict) -> str:\n",
        "        \"\"\"Create an enhanced caption by combining different model outputs\"\"\"\n",
        "        try:\n",
        "            # Get the best basic caption\n",
        "            blip_basic = results.get(\"blip_basic\", \"\")\n",
        "            vit_caption = results.get(\"vit_gpt2\", \"\")\n",
        "            scene_info = results.get(\"clip_scene\", \"\")\n",
        "            confidence = results.get(\"scene_confidence\", 0)\n",
        "\n",
        "            # Choose primary caption based on length and quality indicators\n",
        "            if len(blip_basic) > len(vit_caption) and \"error\" not in blip_basic.lower():\n",
        "                primary_caption = blip_basic\n",
        "            elif \"error\" not in vit_caption.lower():\n",
        "                primary_caption = vit_caption\n",
        "            else:\n",
        "                primary_caption = \"An image with various elements\"\n",
        "\n",
        "            # Add scene context if confident\n",
        "            if confidence > 0.3 and \"error\" not in scene_info.lower():\n",
        "                if \"indoors\" in scene_info:\n",
        "                    primary_caption += \" (indoor setting)\"\n",
        "                elif \"outdoors\" in scene_info:\n",
        "                    primary_caption += \" (outdoor setting)\"\n",
        "                elif \"portrait\" in scene_info:\n",
        "                    primary_caption += \" (portrait style)\"\n",
        "                elif \"landscape\" in scene_info:\n",
        "                    primary_caption += \" (landscape view)\"\n",
        "\n",
        "            return primary_caption\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Enhanced caption generation error: {str(e)}\""
      ],
      "metadata": {
        "id": "cLxI7tEuuBsX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing Caption Generator...\")\n",
        "caption_generator = ImageCaptionGenerator()\n",
        "print(\"Caption Generator ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "b2708faac65d4b1c9e88711d5a585647",
            "5dbe175edd7346aa9c050ae685c8248b",
            "8bb72195cf5d4232a5671c0ed91a5e19",
            "e5f0e4b9efbd4a0895ba110868528597",
            "96dab87db2c74154aa4f42878594ce86",
            "dfbf2adf535940bfb545ba6dc25542af",
            "b9f2b92f7f3145298fb14e668917d617",
            "e5af2a20633b4231b77a09d6a1b22186",
            "a71f3e46ed1f4abca16c864e5bcd5b22",
            "7c6c4a7f93ae4cafadcf8a361e24d58c",
            "8db41a899e044d8881981721af33d16e",
            "4d90d38e0fa848409db0d6cf3dfafaff",
            "a1d9ed6c25be45dd89ef6f327d8d82da",
            "e04966c72a9441929438ac2cefc77184",
            "0be9264b8f084a6680fb0703cebb39dc",
            "9e9e0792daf14383ab2445bfe60cbd7b",
            "1eb5fb7c07a14622a9f95b6f5580ddad",
            "73b745faad5d4e269398c228f4199cf5",
            "3e6eb70f5bd1499bb07f41b68a48ca0f",
            "e0b5a6342269410abc458b8234d6c314",
            "8583597c58f6477fbefe54765abc347f",
            "4d25049ac510490fab4be9d53a62d11d"
          ]
        },
        "id": "GyxgJpCKuEPg",
        "outputId": "10e6a3a2-3d1d-4dc1-dca3-cdfa67e5f876"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Caption Generator...\n",
            "Loading models... This may take a few minutes on first run.\n",
            "Loading BLIP model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2708faac65d4b1c9e88711d5a585647"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ViT-GPT2 model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d90d38e0fa848409db0d6cf3dfafaff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CLIP model...\n",
            "All models loaded successfully!\n",
            "Caption Generator ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# Helper Functions\n",
        "# ====================================\n",
        "def process_image(image):\n",
        "    \"\"\"Process uploaded image and generate captions\"\"\"\n",
        "    if image is None:\n",
        "        return \"Please upload an image first.\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        print(\"Processing image...\")\n",
        "\n",
        "        # Convert to PIL Image if needed\n",
        "        if not isinstance(image, Image.Image):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        # Ensure RGB mode\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Generate comprehensive captions\n",
        "        results = caption_generator.generate_comprehensive_caption(image)\n",
        "\n",
        "        print(\"Caption generation complete!\")\n",
        "\n",
        "        return (\n",
        "            results.get(\"enhanced\", \"Error generating enhanced caption\"),\n",
        "            results.get(\"blip_basic\", \"Error with BLIP basic\"),\n",
        "            results.get(\"blip_conditional\", \"Error with BLIP conditional\"),\n",
        "            results.get(\"vit_gpt2\", \"Error with ViT-GPT2\"),\n",
        "            results.get(\"clip_scene\", \"Error with CLIP scene\"),\n",
        "            f\"Scene Confidence: {results.get('scene_confidence', 0):.2%}\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error processing image: {str(e)}\"\n",
        "        return error_msg, error_msg, error_msg, error_msg, error_msg, error_msg\n",
        "\n",
        "def download_sample_image(url: str) -> Image.Image:\n",
        "    \"\"\"Download a sample image from URL\"\"\"\n",
        "    try:\n",
        "        print(f\"Downloading sample image...\")\n",
        "        response = requests.get(url)\n",
        "        return Image.open(io.BytesIO(response.content))\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading image: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "QJ0uPjS5vCEJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"AI Image Caption Generator\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style=\"text-align: center; padding: 20px;\">\n",
        "            <h1 style=\"color: #2196F3;\">AI Image Caption Generator</h1>\n",
        "            <p style=\"font-size: 18px; color: #666;\">\n",
        "                Upload any image and get intelligent captions from multiple state-of-the-art AI models!\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Image input\n",
        "                image_input = gr.Image(\n",
        "                    label=\"Upload Image\",\n",
        "                    type=\"pil\",\n",
        "                    height=400\n",
        "                )\n",
        "\n",
        "                # Sample images\n",
        "                gr.HTML(\"<h3>Or try these samples:</h3>\")\n",
        "\n",
        "                sample_urls = [\n",
        "                    \"https://dccwebsiteimages.s3.ap-south-1.amazonaws.com/Untitled_design_13_a25f88e21c.png\",  # Dog\n",
        "                    \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\",  # Landscape\n",
        "                    \"https://cookingwithayeh.com/wp-content/uploads/2021/08/Italian-Pizza-1.jpg\",  # Food\n",
        "                ]\n",
        "\n",
        "                with gr.Row():\n",
        "                    sample_btn1 = gr.Button(\"Dog\", size=\"sm\")\n",
        "                    sample_btn2 = gr.Button(\"Landscape\", size=\"sm\")\n",
        "                    sample_btn3 = gr.Button(\"Food\", size=\"sm\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                # Main results\n",
        "                enhanced_output = gr.Textbox(\n",
        "                    label=\"Enhanced Caption (Best Result)\",\n",
        "                    lines=3,\n",
        "                    placeholder=\"Enhanced caption will appear here...\"\n",
        "                )\n",
        "\n",
        "                # Individual model outputs\n",
        "                with gr.Accordion(\"Individual Model Results\", open=False):\n",
        "                    blip_basic_output = gr.Textbox(label=\"BLIP Basic\", lines=2)\n",
        "                    blip_conditional_output = gr.Textbox(label=\"BLIP Conditional\", lines=2)\n",
        "                    vit_gpt2_output = gr.Textbox(label=\"ViT-GPT2\", lines=2)\n",
        "                    clip_scene_output = gr.Textbox(label=\"CLIP Scene Analysis\", lines=2)\n",
        "                    confidence_output = gr.Textbox(label=\"Confidence Score\", lines=1)\n",
        "\n",
        "        # Event handlers\n",
        "        image_input.change(\n",
        "            process_image,\n",
        "            inputs=[image_input],\n",
        "            outputs=[enhanced_output, blip_basic_output, blip_conditional_output,\n",
        "                    vit_gpt2_output, clip_scene_output, confidence_output]\n",
        "        )\n",
        "\n",
        "        # Sample button handlers\n",
        "        sample_btn1.click(\n",
        "            lambda: download_sample_image(sample_urls[0]),\n",
        "            outputs=image_input\n",
        "        )\n",
        "        sample_btn2.click(\n",
        "            lambda: download_sample_image(sample_urls[1]),\n",
        "            outputs=image_input\n",
        "        )\n",
        "        sample_btn3.click(\n",
        "            lambda: download_sample_image(sample_urls[2]),\n",
        "            outputs=image_input\n",
        "        )\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style=\"margin-top: 30px; padding: 20px; background-color: #f5f5f5; border-radius: 10px;\">\n",
        "            <h3>Features:</h3>\n",
        "            <ul>\n",
        "                <li><strong>BLIP</strong>: Salesforce's state-of-the-art vision-language model</li>\n",
        "                <li><strong>ViT-GPT2</strong>: Vision Transformer + GPT-2 for detailed descriptions</li>\n",
        "                <li><strong>CLIP</strong>: OpenAI's model for scene understanding and context</li>\n",
        "                <li><strong>Enhanced Mode</strong>: Combines all models for best results</li>\n",
        "            </ul>\n",
        "            <p><em>Tip: Try different types of images to see how each model performs!</em></p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Create the interface\n",
        "demo = create_interface()"
      ],
      "metadata": {
        "id": "1qThbbKrvDwI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the interface\n",
        "demo.launch(\n",
        "    share=True,  # Creates public link\n",
        "    debug=True,\n",
        "    server_port=7860\n",
        ")\n",
        "\n",
        "print(\"Upload an image to start generating captions!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "MmGcr1ATv1bo",
        "outputId": "36cf8314-453c-415c-a249-03154501a19b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://2f9ae98f3325924bca.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2f9ae98f3325924bca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading sample image...\n",
            "Processing image...\n",
            "Generating BLIP captions...\n",
            "Generating ViT-GPT2 caption...\n",
            "Analyzing scene with CLIP...\n",
            "Creating enhanced caption...\n",
            "Caption generation complete!\n",
            "Downloading sample image...\n",
            "Processing image...\n",
            "Generating BLIP captions...\n",
            "Generating ViT-GPT2 caption...\n",
            "Analyzing scene with CLIP...\n",
            "Creating enhanced caption...\n",
            "Caption generation complete!\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2f9ae98f3325924bca.gradio.live\n",
            "Upload an image to start generating captions!\n"
          ]
        }
      ]
    }
  ]
}